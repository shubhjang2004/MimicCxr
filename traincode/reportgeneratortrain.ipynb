{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOTZfIg7tSKDCn3QJcXFOz7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install opendatasets --q\n","import opendatasets as od\n","od.download(\"https://www.kaggle.com/datasets/shubhamjangid2604/mimiccxr\")\n","od.download(\"https://www.kaggle.com/datasets/shubhamjangid2604/reports\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKt74bXaKmLP","executionInfo":{"status":"ok","timestamp":1760889279373,"user_tz":-330,"elapsed":195293,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}},"outputId":"35c4fd2b-9d35-48a9-e4b9-0fbe96cbc33f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n","Your Kaggle username: \"shubhamjangid2604\"\n","Your Kaggle Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","Dataset URL: https://www.kaggle.com/datasets/shubhamjangid2604/mimiccxr\n","Downloading mimiccxr.zip to ./mimiccxr\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.09G/2.09G [00:25<00:00, 88.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n","Your Kaggle username: \"shubhamjangid2604\"\n","Your Kaggle Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","Dataset URL: https://www.kaggle.com/datasets/shubhamjangid2604/reports\n","Downloading reports.zip to ./reports\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122M/122M [00:00<00:00, 1.51GB/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["!pip install torchxrayvision --q\n","!pip install sacremoses   --q\n","\n"],"metadata":{"id":"TP_gOkeLokFq","executionInfo":{"status":"ok","timestamp":1760890948609,"user_tz":-330,"elapsed":18287,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","import torchxrayvision as xrv\n","import pandas as pd\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from tqdm import tqdm\n","import os"],"metadata":{"id":"WG6KhHr_gCnx","executionInfo":{"status":"ok","timestamp":1760890963373,"user_tz":-330,"elapsed":11627,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","class Config:\n","    # Paths\n","    train_csv = \"train.csv\"\n","    val_csv = \"val.csv\"\n","    save_dir = \"checkpoints\"\n","\n","    # Model\n","    text_model = \"microsoft/biogpt\"\n","\n","    # Training\n","    batch_size = 24\n","    num_epochs = 4\n","    learning_rate = 2e-5\n","    max_length = 128\n","    gradient_accumulation_steps = 2\n","\n","    # Image\n","    image_size = 512\n","    num_visual_tokens = 256\n","    resnet_dim = 2048\n","    biogpt_dim = 1024\n","\n","    # Device\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    # Mixed precision\n","    use_amp = True\n","\n","config = Config()\n","os.makedirs(config.save_dir, exist_ok=True)"],"metadata":{"id":"YtKVmaQWLQjN","executionInfo":{"status":"ok","timestamp":1760890965008,"user_tz":-330,"elapsed":13,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","class MIMICDataset(Dataset):\n","    def __init__(self, csv_file, transform=None):\n","        super().__init__()\n","        self.mimic_df = pd.read_csv(csv_file)\n","        self.transform = transform or transforms.Compose([\n","            transforms.Resize((512, 512)),\n","            transforms.ToTensor(),\n","             # Don't normalize - let TorchXRayVision handle it naturally\n","        ])\n","        print(f\" Loaded {len(self.mimic_df)} samples from {csv_file}\")\n","\n","    def __len__(self):\n","        return len(self.mimic_df)\n","\n","    def __getitem__(self, idx):\n","        row = self.mimic_df.iloc[idx]\n","        report = row[\"report_text\"]\n","        image_path = row[\"image_path\"].replace('\\\\', '/')\n","\n","        # Load image\n","        img = Image.open(image_path).convert(\"L\")  # Grayscale\n","        img = self.transform(img)  # [1, 512, 512]\n","\n","        return {\n","            \"image\": img,\n","            \"report\": report\n","        }\n","\n","def make_collate_fn(tokenizer, max_len=128):\n","    def collate_fn(batch):\n","        texts = [ex[\"report\"] for ex in batch]\n","        images = torch.stack([ex[\"image\"] for ex in batch])\n","\n","        # Tokenize reports\n","        enc = tokenizer(\n","            texts,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=max_len,\n","            return_tensors=\"pt\",\n","            return_attention_mask=True\n","        )\n","\n","        return {\n","            \"images\": images,\n","            \"input_ids\": enc[\"input_ids\"],\n","            \"attention_mask\": enc[\"attention_mask\"]\n","        }\n","    return collate_fn\n"],"metadata":{"id":"yJXfsFg9LVLD","executionInfo":{"status":"ok","timestamp":1760890967343,"user_tz":-330,"elapsed":9,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","class ImageEncoder(nn.Module):\n","    def __init__(self, xrv_model, resnet_dim=2048, biogpt_dim=1024):\n","        super().__init__()\n","        # Feature extractor (gives [B, 2048, 16, 16])\n","        self.feature_extractor = nn.Sequential(\n","            *list(xrv_model.model.children())[:-2]\n","        )\n","\n","        # Project 2048 â†’ 1024\n","        self.projection = nn.Sequential(\n","            nn.Linear(resnet_dim, biogpt_dim),\n","            nn.LayerNorm(biogpt_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.1)\n","        )\n","\n","    def forward(self, images):\n","        # Extract features\n","        features = self.feature_extractor(images)  # [B, 2048, 16, 16]\n","\n","        # Reshape to tokens\n","        B, C, H, W = features.shape\n","        tokens = features.flatten(2).transpose(1, 2)  # [B, 256, 2048]\n","\n","        # Project\n","        tokens = self.projection(tokens)  # [B, 256, 1024]\n","\n","        return tokens\n"],"metadata":{"id":"JYo56qymLdPd","executionInfo":{"status":"ok","timestamp":1760890970806,"user_tz":-330,"elapsed":11,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","class MedicalReportGenerator(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        # Load image encoder\n","        print(\"Loading ResNet50 from TorchXRayVision...\")\n","        xrv_resnet = xrv.models.ResNet(weights=\"resnet50-res512-all\")\n","        self.image_encoder = ImageEncoder(\n","            xrv_resnet,\n","            resnet_dim=config.resnet_dim,\n","            biogpt_dim=config.biogpt_dim\n","        )\n","\n","        # Load text decoder\n","        print(\"Loading BioGPT...\")\n","        self.text_decoder = AutoModelForCausalLM.from_pretrained(config.text_model)\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"MODEL ARCHITECTURE\")\n","        print(\"=\"*70)\n","        print(f\"Image Encoder:   ResNet50-res512-all (medical pretrained)\")\n","        print(f\"Visual Tokens:   {config.num_visual_tokens}\")\n","        print(f\"Token Dimension: {config.resnet_dim} â†’ {config.biogpt_dim}\")\n","        print(f\"Text Decoder:    BioGPT\")\n","        print(\"=\"*70 + \"\\n\")\n","\n","    def forward(self, images, input_ids, attention_mask):\n","        \"\"\"\n","         Training forward pass using two-pass method with caching\n","\n","         Args:\n","        images: [B, 1, 512, 512]\n","        input_ids: [B, T] - tokenized reports\n","        attention_mask: [B, T] - attention mask for reports\n","\n","       Returns:\n","        loss: scalar\n","        \"\"\"\n","        B = images.shape[0]\n","        V = self.config.num_visual_tokens  # 256\n","\n","    # ============================================================\n","    # PASS 1: Encode images and cache\n","    # ============================================================\n","\n","    # Get image embeddings\n","        image_embeds = self.image_encoder(images)  # [B, 256, 1024]\n","\n","    # Create prefix attention mask (all ones - all image tokens visible)\n","        prefix_mask = torch.ones(B, V, dtype=torch.long, device=images.device)\n","\n","    # Forward through decoder to cache image context\n","    # Use no_grad to save memory (we only want the cache)\n","        with torch.no_grad():\n","            out_prefix = self.text_decoder(\n","                inputs_embeds=image_embeds,\n","                attention_mask=prefix_mask,\n","                use_cache=True,\n","                output_hidden_states=False\n","            )\n","        past_kv = out_prefix.past_key_values\n","\n","    # ============================================================\n","    # PASS 2: Generate text using cached image context\n","    # ============================================================\n","\n","    # Combined attention mask: image prefix + text\n","    # This tells the model: \"text tokens can see image tokens + previous text\"\n","        combined_mask = torch.cat([prefix_mask, attention_mask], dim=1)  # [B, V+T]\n","\n","    # Labels: just the input_ids (model will shift internally for next-token prediction)\n","        labels = input_ids.clone()  # [B, T]\n","\n","    # Forward through text decoder with cached image context\n","        outputs = self.text_decoder(\n","            input_ids=input_ids,           # [B, T] - text tokens to process\n","            attention_mask=combined_mask,  # [B, V+T] - can attend to image + text\n","            past_key_values=past_kv,       # Cached K/V for image tokens\n","            labels=labels,                 # [B, T] - targets for next-token prediction\n","            use_cache=True\n","        )\n","\n","        return outputs.loss\n","\n","    @torch.no_grad()\n","    def generate(self, images, max_length=128, num_beams=4):\n","        \"\"\"\n","        Generate reports from images\n","\n","        Args:\n","            images: [B, 1, 512, 512]\n","            max_length: maximum tokens to generate\n","            num_beams: beam search width\n","\n","        Returns:\n","            generated_ids: [B, max_length]\n","        \"\"\"\n","        self.eval()\n","\n","        # Encode images\n","        image_embeds = self.image_encoder(images)  # [B, 256, 1024]\n","\n","        # Generate from image embeddings\n","        generated_ids = self.text_decoder.generate(\n","            inputs_embeds=image_embeds,\n","            max_length=max_length,\n","            num_beams=num_beams,\n","            early_stopping=True,\n","            pad_token_id=self.text_decoder.config.pad_token_id,\n","            eos_token_id=self.text_decoder.config.eos_token_id\n","        )\n","\n","        return generated_ids\n"],"metadata":{"id":"dzQdkVx1LiTJ","executionInfo":{"status":"ok","timestamp":1760890973049,"user_tz":-330,"elapsed":63,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, dataloader, optimizer, scaler, config, epoch):\n","    model.train()\n","    total_loss = 0\n","    progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n","\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(progress):\n","        # Move to device\n","        images = batch[\"images\"].to(config.device)\n","        input_ids = batch[\"input_ids\"].to(config.device)\n","        attention_mask = batch[\"attention_mask\"].to(config.device)\n","\n","        # Forward pass with mixed precision\n","        if config.use_amp:\n","            with autocast():\n","                loss = model(images, input_ids, attention_mask)\n","                loss = loss / config.gradient_accumulation_steps\n","\n","            # Backward pass\n","            scaler.scale(loss).backward()\n","\n","            # Update weights every gradient_accumulation_steps\n","            if (step + 1) % config.gradient_accumulation_steps == 0:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","        else:\n","            loss = model(images, input_ids, attention_mask)\n","            loss = loss / config.gradient_accumulation_steps\n","            loss.backward()\n","\n","            if (step + 1) % config.gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","        # Track loss\n","        total_loss += loss.item() * config.gradient_accumulation_steps\n","        progress.set_postfix({'loss': loss.item() * config.gradient_accumulation_steps})\n","\n","    avg_loss = total_loss / len(dataloader)\n","    return avg_loss\n","\n","@torch.no_grad()\n","def validate(model, dataloader, config):\n","    model.eval()\n","    total_loss = 0\n","\n","    progress = tqdm(dataloader, desc=\"Validation\")\n","    for batch in progress:\n","        images = batch[\"images\"].to(config.device)\n","        input_ids = batch[\"input_ids\"].to(config.device)\n","        attention_mask = batch[\"attention_mask\"].to(config.device)\n","\n","        if config.use_amp:\n","            with autocast():\n","                loss = model(images, input_ids, attention_mask)\n","        else:\n","            loss = model(images, input_ids, attention_mask)\n","\n","        total_loss += loss.item()\n","        progress.set_postfix({'loss': loss.item()})\n","\n","    avg_loss = total_loss / len(dataloader)\n","    return avg_loss"],"metadata":{"id":"xEs8yB36LpZx","executionInfo":{"status":"ok","timestamp":1760890976345,"user_tz":-330,"elapsed":17,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","def main():\n","    print(\"=\"*70)\n","    print(\"MEDICAL REPORT GENERATION TRAINING\")\n","    print(\"=\"*70)\n","\n","    # Load tokenizer\n","    print(\"\\nLoading tokenizer...\")\n","    tokenizer = AutoTokenizer.from_pretrained(config.text_model)\n","\n","    # Set pad token if not present\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Load datasets\n","    print(\"\\nLoading datasets...\")\n","    train_dataset = MIMICDataset(config.train_csv)\n","    val_dataset = MIMICDataset(config.val_csv)\n","\n","    # Create dataloaders\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        shuffle=True,\n","        num_workers=4,\n","        collate_fn=make_collate_fn(tokenizer, config.max_length),\n","        pin_memory=True\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=config.batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        collate_fn=make_collate_fn(tokenizer, config.max_length),\n","        pin_memory=True\n","    )\n","\n","    print(f\"Train batches: {len(train_loader)}\")\n","    print(f\"Val batches: {len(val_loader)}\")\n","\n","    # Initialize model\n","    print(\"\\nInitializing model...\")\n","    model = MedicalReportGenerator(config).to(config.device)\n","\n","    # Count parameters\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"\\nTotal parameters: {total_params:,}\")\n","    print(f\"Trainable parameters: {trainable_params:,}\")\n","\n","    # Optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n","\n","    # Mixed precision scaler\n","    scaler = GradScaler() if config.use_amp else None\n","\n","    # Training loop\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"STARTING TRAINING\")\n","    print(\"=\"*70 + \"\\n\")\n","\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(config.num_epochs):\n","        # Train\n","        train_loss = train_epoch(model, train_loader, optimizer, scaler, config, epoch)\n","        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}\")\n","\n","        # Validate\n","        val_loss = validate(model, val_loader, config)\n","        print(f\"Epoch {epoch+1} - Val Loss: {val_loss:.4f}\")\n","\n","        # Save best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            checkpoint_path = os.path.join(config.save_dir, \"best_model.pt\")\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'train_loss': train_loss,\n","                'val_loss': val_loss,\n","            }, checkpoint_path)\n","            print(f\" Saved best model (val_loss: {val_loss:.4f})\")\n","\n","        # Save checkpoint every epoch\n","        checkpoint_path = os.path.join(config.save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'train_loss': train_loss,\n","            'val_loss': val_loss,\n","        }, checkpoint_path)\n","\n","        print(\"-\"*70 + \"\\n\")\n","\n","    print(\"=\"*70)\n","    print(\"TRAINING COMPLETE!\")\n","    print(\"=\"*70)\n","    print(f\"Best validation loss: {best_val_loss:.4f}\")\n","\n","# ==============================================================================\n","# INFERENCE EXAMPLE\n","# ==============================================================================\n","\n","def inference_example():\n","    \"\"\"Example of how to generate reports from images\"\"\"\n","\n","    # Load model\n","    model = MedicalReportGenerator(config).to(config.device)\n","    checkpoint = torch.load(os.path.join(config.save_dir, \"best_model.pt\"))\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    # Load tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(config.text_model)\n","\n","    # Load a sample image\n","    val_dataset = MIMICDataset(config.val_csv)\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=1,\n","        shuffle=False,\n","        collate_fn=make_collate_fn(tokenizer, config.max_length)\n","    )\n","\n","    # Generate\n","    batch = next(iter(val_loader))\n","    images = batch[\"images\"].to(config.device)\n","\n","    print(\"Generating report...\")\n","    generated_ids = model.generate(images, max_length=config.max_length, num_beams=4)\n","\n","    # Decode\n","    generated_report = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    ground_truth = tokenizer.decode(batch[\"input_ids\"][0], skip_special_tokens=True)\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"GENERATED REPORT:\")\n","    print(\"=\"*70)\n","    print(generated_report)\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"GROUND TRUTH:\")\n","    print(\"=\"*70)\n","    print(ground_truth)\n","\n"],"metadata":{"id":"8HC5cmpzMY9w","executionInfo":{"status":"ok","timestamp":1760890983696,"user_tz":-330,"elapsed":19,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# RUN\n","# ==============================================================================\n","\n","if __name__ == \"__main__\":\n","    # Train\n","    main()\n","\n","    # Inference example\n","    # inference_example()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"icTXH218Mrac","executionInfo":{"status":"ok","timestamp":1760904697771,"user_tz":-330,"elapsed":13710914,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}},"outputId":"a6177482-826f-4e05-bb63-8f25f0ca0b8f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","MEDICAL REPORT GENERATION TRAINING\n","======================================================================\n","\n","Loading tokenizer...\n","\n","Loading datasets...\n"," Loaded 94949 samples from train.csv\n"," Loaded 10526 samples from val.csv\n","Train batches: 3957\n","Val batches: 439\n","\n","Initializing model...\n","Loading ResNet50 from TorchXRayVision...\n","Loading BioGPT...\n","\n","======================================================================\n","MODEL ARCHITECTURE\n","======================================================================\n","Image Encoder:   ResNet50-res512-all (medical pretrained)\n","Visual Tokens:   256\n","Token Dimension: 2048 â†’ 1024\n","Text Decoder:    BioGPT\n","======================================================================\n","\n","\n","Total parameters: 372,365,248\n","Trainable parameters: 372,365,248\n","\n","======================================================================\n","STARTING TRAINING\n","======================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3957/3957 [51:43<00:00,  1.28it/s, loss=3.51]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Train Loss: 5.2404\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439/439 [04:00<00:00,  1.83it/s, loss=3.79]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Val Loss: 3.5402\n"," Saved best model (val_loss: 3.5402)\n","----------------------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3957/3957 [51:41<00:00,  1.28it/s, loss=3.47]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 - Train Loss: 3.4267\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439/439 [03:59<00:00,  1.83it/s, loss=3.06]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 - Val Loss: 2.8507\n"," Saved best model (val_loss: 2.8507)\n","----------------------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3957/3957 [51:39<00:00,  1.28it/s, loss=2.46]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 - Train Loss: 2.8776\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439/439 [03:59<00:00,  1.83it/s, loss=2.66]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 - Val Loss: 2.4683\n"," Saved best model (val_loss: 2.4683)\n","----------------------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3957/3957 [51:42<00:00,  1.28it/s, loss=2.49]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 - Train Loss: 2.5404\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439/439 [03:59<00:00,  1.83it/s, loss=2.41]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 - Val Loss: 2.2326\n"," Saved best model (val_loss: 2.2326)\n","----------------------------------------------------------------------\n","\n","======================================================================\n","TRAINING COMPLETE!\n","======================================================================\n","Best validation loss: 2.2326\n"]}]},{"cell_type":"code","source":["try:\n","    from google.colab import files\n","    print(\"\\n\" + \"=\"*70)\n","    print(\" DOWNLOADING BEST CHECKPOINT\")\n","    print(\"=\"*70)\n","    files.download('checkpoints/best_model.pt')\n","    print(\" Downloaded best_model.pt (1.7GB)\")\n","    print(\"=\"*70)\n","except:\n","    print(\" Not in Colab - checkpoint saved locally\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138},"id":"sfwp7-z4VS4t","executionInfo":{"status":"ok","timestamp":1760904698026,"user_tz":-330,"elapsed":31,"user":{"displayName":"placementbyclaude2025","userId":"09151736546609362087"}},"outputId":"273b3546-319c-4623-abc1-32b3f281c665"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","ðŸ“¥ DOWNLOADING BEST CHECKPOINT\n","======================================================================\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5673bb3e-8a75-4137-9665-16774abde5b7\", \"best_model.pt\", 1846448077)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Downloaded best_model.pt (1.6GB)\n","======================================================================\n"]}]}]}